\begin{tabular}{llll}
\toprule
\textbf{Compression Method} & \textbf{Inference Time CPU} & \textbf{Inference Time GPU} & \textbf{Model Size} \\
\midrule
None & 6.047 (sec) & 0.010 (sec) & 499 (MB) \\
Know. Distil. & -84.7\% $\pm$ 0.7\% & -29.2\% $\pm$ 0.5\% & -48.7\% \\
Pruning (0.2) & 5.5\% $\pm$ 1.0\% & 11.1\% $\pm$ 0.7\% & 0.0\% \\
Pruning (0.4) & 16.1\% $\pm$ 2.2\% & 6.4\% $\pm$ 1.4\% & 0.0\% \\
Pruning (0.6) & 3.1\% $\pm$ 3.5\% & 19.8\% $\pm$ 3.3\% & 0.0\% \\
Quantization (float8) & 34.2\% $\pm$ 2.9\% & 113.2\% $\pm$ 1.8\% & -51.4\% \\
Quantization (int4) & 61.8\% $\pm$ 4.8\% & 209.6\% $\pm$ 2.0\% & -59.3\% \\
Quantization (int8) & 42.2\% $\pm$ 3.8\% & 105.9\% $\pm$ 1.1\% & -51.4\% \\
\bottomrule
\end{tabular}
